{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "- The purpose of text classification is to automatically classify texts or documents into one or more predefined categories. \n",
    "\n",
    "- From a business perspective, text classification can help understand the sentiment of social media users, identify spam and normal mail, automatically mark users' queries, classify news by existing topics, and so on. \n",
    "\n",
    "- From the programming view, text classification is focusing on training a model to predict the new unlabeled texts/documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "- This assignment provides consumer reviews of 5 categories, they are Automotive (1455 reviews), Bars (1460 reviews), Health and medical (1450 reviews), Hotels and travel (1430 reviews), and Restaurants (1440 reviews), which are available at http://mlg.ucd.ie/modules/yalp/.\n",
    "\n",
    "- Each review has a star rating. For this assignment, assuming that 1-star to 3-star reviews are “negative”, and 4-star to 5-star reviews as “positive”.\n",
    "\n",
    "- The objective of this assignment is to scrape consumer reviews from a set of web pages and evaluate the performance of text classification on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Decomposition\n",
    "#### There are three main tasks for this assignment:\n",
    "\n",
    "***Task 1:  Scrape all reviews and store them***\n",
    " - Scrape reviews and parse the html page.\n",
    " - Use the number of stars to label the reviews. \n",
    " - Store the review text and the class label.\n",
    "\n",
    "***Task 2:  Create a numeric representation of data and build the classifier***\n",
    " - Apply appropriate preprocessing steps and find proper numeric representations.\n",
    " - Build a classification model.\n",
    " - Evaluate the model.\n",
    "\n",
    "***Task 3:  Evaluate the model's ability of transfering between two categories***\n",
    " - Train a classification model on the data from “Category A”, and evaluate its performance on the data from “Category B”.\n",
    " - Train a classification model on the data from “Category B”, and evaluate its performance on the data from “Category A”.\n",
    " \n",
    "***Read me before executing the code:***\n",
    " - In order to avoid unnecessary errors when the program runs, please execute the code in sequence, it is best not to skip a code segment to execute.\n",
    " - Training models are involved in this project, so it may take some time to get results when executing, please be patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "In this project, we are trying to compare the performance of different classifiers on a dataset, then training to get the best model to apply this model on another dataset to show its performance.\n",
    "\n",
    "My hypothesis is that a classifier model has similar performance on a similar dataset, so I choose two similar categories of reviews: **Bars and Restaurants**, which may both include reviews of food, location, services,  environment and so on.\n",
    "\n",
    "I will create 5 classification models to predict class labels by applying KNN classifier, Decision Tree classifier, Naive Bayes classifier, SVM classifier, and Logistic Regression classifier to find the best model among them. \n",
    "\n",
    "First, import all packages needed and scrape all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class reviewSpider to scrape reviews.\n",
    "class reviewSpider(object):\n",
    "\n",
    "    def __init__(self, start_url, category, csv_path='data.csv'):\n",
    "        self.start_url = start_url  # start_url is http://mlg.ucd.ie/modules/yalp/\n",
    "        self.category = category  # one category that been chosen\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'\n",
    "        }\n",
    "        self.text = []  # store all reviews under this category\n",
    "        self.labels = []  # label the data by the number of stars\n",
    "        self.csv_path = csv_path  # the path to store csv file\n",
    "\n",
    "    # 1. Return the object processed by BeautifulSoup,\n",
    "    # and grab the hyperlink of each category (getCategoryURL) based on the parsed content.\n",
    "    # Beautiful Soup provides functions for handling navigation, searching, modifying parse trees, and so on. \n",
    "    # It's a toolbox that provides users with the data they need to crawl by parsing the document and it's simple.\n",
    "    def parseHTML(self, url):\n",
    "        '''\n",
    "        :param url: input the url which needs to be parsed\n",
    "        '''\n",
    "        r = requests.get(url, headers=self.headers)\n",
    "        html = r.content.decode('utf-8')\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    # 2. According to the hyperlink of each category, get into the hyperlink, parse the webpage, \n",
    "    # and fetch the hyperlink (generateBusinessURL) of each business(bar/restaurant) based on the parsed content.\n",
    "    def getCategoryURL(self):\n",
    "        '''\n",
    "        get the category start url\n",
    "        :return: the categorie's start URL\n",
    "        '''\n",
    "        self.category_dict = {}\n",
    "        soup = self.parseHTML(self.start_url)\n",
    "        # By checking the source code, each category is stored in the h4 tag.\n",
    "        for h in soup.find_all('h4'): \n",
    "            self.category_dict[h.a.text.split(': ')[1]] = self.start_url + h.a['href']\n",
    "        return self.category_dict[self.category]\n",
    "\n",
    "    # 3. According to the hyperlink of each business, get into the hyperlink, parse the webpage, \n",
    "    # grab and store(store_csv) each of the comments and number of stars \n",
    "    # in the business(bar/restaurant) according to the parsed content.\n",
    "    def generateBusinessURL(self, categoryURL):\n",
    "        '''\n",
    "        from the Category URL, generate Business URL\n",
    "        :return: yield Business URLs\n",
    "        '''\n",
    "        soup = self.parseHTML(categoryURL)\n",
    "        # Each business(bar/restaurant) is stored in the h6 tag.\n",
    "        for h in soup.find_all('h6'): \n",
    "            # for check and debug, check the progress of crawling reviews.\n",
    "            print(h.text)  \n",
    "            yield self.start_url + h.a['href']\n",
    "\n",
    "    def store_csv(self):\n",
    "        # As DataFrame is \"a 2-dimensional labelled data structure with columns of data that can be of different types\".\n",
    "        # So I store the data as the DataFrame structure for the purpose of Task 2,\n",
    "        # displaying data in a numerical way.\n",
    "        df = pd.DataFrame()\n",
    "        df['review'] = self.text\n",
    "        df['label'] = self.labels\n",
    "        df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "    def run(self):\n",
    "        categoryURL = self.getCategoryURL()\n",
    "        for url in self.generateBusinessURL(categoryURL):\n",
    "            soup = self.parseHTML(url)\n",
    "            # Find the content in the review div.\n",
    "            for d in soup.find_all('div', {'class': \"review\"}):\n",
    "                # Get the image name under the alt tag. (1-star, 2-star...)\n",
    "                star_num = int(d.p.next_sibling.img['alt'][0]) \n",
    "                # Specify what the label represents.\n",
    "                self.labels.append(1 if star_num > 3 else 0)\n",
    "                # 1-star to 3-star are “negative”, and 4-star to 5-star reviews as “positive”. \n",
    "                # Determine that \"positive\" is stored as 1, \"negative\" is 0.\n",
    "                text = d.p.next_sibling.next_sibling.text\n",
    "                self.text.append(text)\n",
    "        self.store_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bars\n",
      "1. Applebee's Neighborhood Grill & Bar\n",
      "2. Bar George\n",
      "3. Barrel Grill & Modern Saloon\n",
      "4. Blaqcat Ultra Hookah Lounge\n",
      "5. Blu Burger Grille\n",
      "6. Boca Taqueria\n",
      "7. Boondocks Patio & Grill\n",
      "8. Boston Pizza\n",
      "9. Buck & Badger\n",
      "10. Buffalo Wild Wings\n",
      "11. Cabin Fever\n",
      "12. California Pizza Kitchen at Summerlin\n",
      "13. Chalker's Pub Billiards & Bistro\n",
      "14. Chili's\n",
      "15. Condado Tacos\n",
      "16. Dark Horse Sports Bar & Grill\n",
      "17. Ellis Island Hotel, Casino & Brewery\n",
      "18. Fly 2.0\n",
      "19. Furco\n",
      "20. Game Time Sports Grill\n",
      "21. George & Dragon II English Restaurant Pub\n",
      "22. Goldwater Brewing\n",
      "23. Gray's Tied House\n",
      "24. JangBang Bar&Grill\n",
      "25. Jodi B's Restaurant\n",
      "26. Kirks Korner\n",
      "27. La Piñata\n",
      "28. Latitude 360\n",
      "29. Laziza Hookah Lounge & Restaurant\n",
      "30. Le Diner Frunchroom\n",
      "31. Let's Be Frank\n",
      "32. Mac's Speed Shop\n",
      "33. Midtown\n",
      "34. Outback Steakhouse\n",
      "35. Paloma Family Restaurant\n",
      "36. Park Place Pub\n",
      "37. Pepper's Cafe\n",
      "38. Piazza Lounge\n",
      "39. Pireas\n",
      "40. Primanti Bros\n",
      "41. Primo Tuscan Grille\n",
      "42. Roma Italian Restaurant and Pizzeria\n",
      "43. Saltlik Calgary\n",
      "44. Scorpion Bay Grill\n",
      "45. Scruffy Murphy's Irish Pub & Restaurant\n",
      "46. Scullery\n",
      "47. Shamrock Bar & Grille\n",
      "48. Sotto\n",
      "49. St. Louis Bar & Grill\n",
      "50. Station HO.ST\n",
      "51. Sundance 608\n",
      "52. TRH-Bar\n",
      "53. Texas Land & Cattle\n",
      "54. Thai Basil Signature\n",
      "55. The Berwick Public House\n",
      "56. The Blind Pig\n",
      "57. The Dillinger\n",
      "58. The Duchess of Markham\n",
      "59. The Fox & Fiddle\n",
      "60. The Great Dane\n",
      "61. The Keg Steakhouse + Bar - Pointe-Claire\n",
      "62. The Wine Spot\n",
      "63. Tuck Shop\n",
      "64. Twin Oaks Lounge\n",
      "65. Village Martini & Wine Bar\n",
      "66. Walnut Grill-Wexford\n",
      "67. Yucca Tap Room\n",
      "68. Zen Culinary\n",
      "Restaurants\n",
      "1. Au Festin de Babette\n",
      "2. BV Pub and Pizzeria\n",
      "3. Barros Pizza\n",
      "4. Beer Garden\n",
      "5. Biaggio's Pizzeria\n",
      "6. Blackfinn Ameripub\n",
      "7. Buford's Kitchen\n",
      "8. Canis\n",
      "9. Cannery Row Buffet\n",
      "10. Carbonara's Restaurant\n",
      "11. Chick-fil-A\n",
      "12. Cocina Oaxaqueña\n",
      "13. Crystal & Jules\n",
      "14. Di Manno Bakery\n",
      "15. Express Pizzeria\n",
      "16. Famous Gyro George\n",
      "17. Forum Cafe\n",
      "18. GreenMix\n",
      "19. Ground Zero\n",
      "20. Hogs and Hops\n",
      "21. Ichiban Fish House\n",
      "22. Jimmy John's\n",
      "23. Joey & Gina's Restaurant\n",
      "24. John Michael's Pub\n",
      "25. Joy Thai Restaurant\n",
      "26. Kabab Palace\n",
      "27. Le Fumoir Rubs\n",
      "28. Lola's\n",
      "29. LongHorn Steakhouse\n",
      "30. Lyndhurst Grill by J. Alexander's\n",
      "31. Mandarin Chef\n",
      "32. Mangiacake Ristorante & Catering\n",
      "33. Milano Italian Restaurant\n",
      "34. Mirch Masala\n",
      "35. Mr Pho\n",
      "36. Nando's Flame Grilled Chicken\n",
      "37. Narong's Thai Kitchen\n",
      "38. Nest Restaurant & Lounge\n",
      "39. New Day Cafe\n",
      "40. Oaxaca Restaurante Y Cantina\n",
      "41. OddFellows\n",
      "42. Orient House\n",
      "43. Philly Pretzel Factory\n",
      "44. Pho Tién Phát\n",
      "45. Pita House\n",
      "46. Pita Jungle\n",
      "47. Pizza Picazzo, LLC\n",
      "48. Piñon Grill\n",
      "49. Resto-Bar La Belle Province\n",
      "50. Ricky's All Day Grill\n",
      "51. Rocket Burger & Subs\n",
      "52. S&D Cafe\n",
      "53. Smokin Thyme Kitchen\n",
      "54. Solasta\n",
      "55. Star Ginger\n",
      "56. Sujeo\n",
      "57. Thai Home\n",
      "58. The Bier Markt\n",
      "59. The Captain's Boil\n",
      "60. The Farmer's House\n",
      "61. The Human Bean\n",
      "62. The Keg Steakhouse + Bar - Macleod Trail\n",
      "63. Tokyo Stop\n",
      "64. Ultimate Cafe\n",
      "65. Villa Pizza\n",
      "66. Villa Risi\n",
      "67. Vivo Ristorante\n"
     ]
    }
   ],
   "source": [
    " # Create a reviewSpider object. \n",
    " # Instantiate a reviewSpider object for each category selected, crawl and store the reviews and their labels. \n",
    " # Print names of the bar/restaurant under the category.\n",
    "start_url = 'http://mlg.ucd.ie/modules/yalp/'\n",
    "categories = ['Bars', 'Restaurants']\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    # Store review and their lables whith a file name \"category(bars/restaurants).csv\"\n",
    "    rs = reviewSpider(start_url, category, csv_path = category + '.csv') \n",
    "    rs.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "#### From the reviews in this category, apply appropriate preprocessing steps to create a numeric representation of the data, suitable for classification.\n",
    "\n",
    "Printing the reviews that scraped in Task1 and found that all the content of a piece review is stored in a cell.\n",
    "\n",
    "As we want to classify the labels of reviews, a good way to represent the text is to build word vector, \"make words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0\"(Dhruvil, 2018). \n",
    "\n",
    "The word vector provides a mathematical method of transforming symbolic information in natural language into digital information in vector form. This turns the problem of natural language understanding into a problem of data science.\n",
    "\n",
    "Word2vec is a computationally efficient predictive model for learning word embedding in raw text. It is divided into two types: the continuous word bag model (CBOW) and the Skip-Gram model.\n",
    "\n",
    "Another tool to build word vector is Doc2Vec, which is similar to Word2Vec, it adds a paragraph vector under the basis of word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bars\n",
    "***In order to distinguish Bars and Restaurants, I separated the implementation of them.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It would help if the front girl don't just sit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One star because that's the least amount you c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bad time today. Dirty windows, table sticky, h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My second visit in the last year. Both experie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely awful! Took forever to get food, fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  It would help if the front girl don't just sit...      0\n",
       "1  One star because that's the least amount you c...      0\n",
       "2  Bad time today. Dirty windows, table sticky, h...      0\n",
       "3  My second visit in the last year. Both experie...      0\n",
       "4  Absolutely awful! Took forever to get food, fo...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five pieces of reviews and their labels, all the content of a piece review is stored in a cell.\n",
    "data_bars = pd.read_csv(\"Bars.csv\")\n",
    "data_bars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Pre-processing steps:***\n",
    "\n",
    "1) Tokenize: Split each sentence into a series of words.\n",
    "\n",
    "2) Normalization: Remove all non-alphanumeric letters and convert uppercase letters to lowercase letters.\n",
    "\n",
    "From a computer perspective, it cannot distinguish the different meanings between 'Car', ‘car’, and 'CAR', so we generally convert all the letters in the text to lowercase or uppercase (usually lowercase).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'would', 'help', 'if', 'the', 'front', 'girl', 'don', ' ', 't', 'just', 'sit', 'us', 'down', 'and', 'not', 'ask', 'us', 'for', 'drinks', 'or', 'put', 'us', 'with', 'a', 'waitress', ' ', 'instead', ' ', 'let', 'us', 'just', 'sit', 'here', 'for', 'almost', 'an', 'hour', 'unattended', ' ', 'if', 'it', ' ', 's', 'time', 'for', 'you', 'to', 'clock', 'out', 'then', 'it', ' ', 's', 'not', 'our', 'problem', ' ', 'but', 'if', 'you', ' ', 're', 'gonna', 'seat', 'us', 'be', 'more', 'professional', 'about', 'it', 'at', 'least', 'then', 'leave', ' ']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the reviews and build a corpus.\n",
    "tokenizer_bars = WordPunctTokenizer()\n",
    "corpus = []\n",
    "for rev in data_bars.review:\n",
    "    cor = []\n",
    "    for ele in tokenizer_bars.tokenize(rev):\n",
    "        # Normalization: Use the sub regex of the re module to match all non-alphanumeric letters,\n",
    "        # and replace them with spaces.\n",
    "        ele = re.sub(r'[^a-zA-Z0-9]', \" \", ele)\n",
    "        # Normalization: Convert uppercase letters to lowercase letters.\n",
    "        cor.append(ele.lower())\n",
    "    corpus.append(cor)\n",
    "# Check the results of tokenization.\n",
    "print(corpus[0: 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why I didn't remove stop words, Stem and Lemmatize the text is that it might get a high accuracy but we don't know the reason is the text itself or the classification we build caused the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Numeric representation***\n",
    "\n",
    "Here I use gensim toolkit in Doc2Vec to build word vectors, instead of Word2Doc. Doc2vec is an unsupervised learning algorithm, which is used to predict a vector to represent different documents and doesn't require a sentence with a fixed length. The structure of this model potentially overcomes the disadvantages of the word bag model.\n",
    "\n",
    "\"In a plain Word2Vec model the word would have exactly the same representation in both sentences, in Doc2Vec it will not.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bars_path = 'review_bars.model'\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "model = Doc2Vec(documents, vector_size=64, window=3, min_count=1, workers=4)\n",
    "# The explanation of these parameters in the official document:\n",
    "# documents: Input corpus\n",
    "# vector_size: Dimensionality of the feature vectors.\n",
    "# window: The maximum distance between the current and predicted word within a sentence.\n",
    "# min_count: Ignores all words with total frequency lower than this.\n",
    "# workers: Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "# I use the default settings here. But it is necessary to test the best parameter settings.\n",
    "model.save(model_bars_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Build classification models***\n",
    "\n",
    "I choose 5 classifiers, and applying GridSearchCV to find the best classifier with the best parameters. \n",
    "\n",
    "The simple grid search process is: after the original data set is divided into a training set and a test set, the test set is used to measure the quality of the model in addition to being used as an adjustment parameter; this results in a final scoring result that is better than the actual result. Because the test set is sent to the model during the tuning process, and our goal is to apply the training model to unseen data; cross-validation can be used to handle this problem, so in this project, I use an approach combing cross-validation and grid search here, GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bars = []\n",
    "for review in corpus:\n",
    "    X_bars.append(model.infer_vector(review))\n",
    "X_bars = np.array(X_bars)\n",
    "y_bars = data_bars.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1) KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "# The n_neighbors of KNN generally take odd numbers, \n",
    "# in order to avoid the case where the classification result of the neighboring node \n",
    "# is half and half when the even number is selected, and then the classification result needs to be randomly selected.\n",
    "params = {'n_neighbors': [2 * i + 1 for i in range(8)]}  \n",
    "# The optimal hyperparameter combination is obtained by \n",
    "# calculating the average accuracy of the 5-fold cross-validation under different hyperparameter combinations.\n",
    "clf_knn_bars = GridSearchCV(knn, params, cv=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6678082191780822\n",
      "{'n_neighbors': 9}\n"
     ]
    }
   ],
   "source": [
    "clf_knn_bars.fit(X_bars, y_bars)\n",
    "# Print the average accuracy of cross-validation of the optimal model.\n",
    "print(clf_knn_bars.best_score_)  \n",
    "# Print the hyperparameter of the optimal model.\n",
    "print(clf_knn_bars.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# kf = KFold(n_splits=5)\n",
    "# X_train = X_bars;\n",
    "# y_train = y_bars;\n",
    "# for train_index, test_index in kf.split(X_train):\n",
    "#   X_t, X_s = X_train[train_index], X_train[test_index]\n",
    "#   y_t, y_s = y_train[train_index], y_train[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2) Decision Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier()\n",
    "params = {'max_depth': [2 * i + 1 for i in range(8)]}\n",
    "clf_dtc_bars = GridSearchCV(dtc, params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6719178082191781\n",
      "{'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "clf_dtc_bars.fit(X_bars, y_bars)\n",
    "print(clf_dtc_bars.best_score_)\n",
    "print(clf_dtc_bars.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3) Naive Bayes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6424657534246576\n"
     ]
    }
   ],
   "source": [
    "clf_gnb = GaussianNB()\n",
    "scores = cross_val_score(clf_gnb, X_bars, y_bars, cv=5)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4) SVM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(gamma='auto')\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly'), 'C': [1, 10, 100, 150], 'degree': [2, 3, 4]}\n",
    "# 'linear', 'rbf' and 'poly' are most frequently used.\n",
    "# C indicates the penalty coefficient of the regular term, and degree only works when the kernel is poly, \n",
    "# indicating several polynomials.\n",
    "clf_svm_bars = GridSearchCV(svc, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7280821917808219\n",
      "{'C': 150, 'degree': 2, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "clf_svm_bars.fit(X_bars, y_bars)\n",
    "print(clf_svm_bars.best_score_)\n",
    "print(clf_svm_bars.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5) Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = linear_model.LogisticRegression()\n",
    "parameters = {'penalty': ['l1', 'l2'], 'C': [1, 10, 100, 150]}\n",
    "# Penalty indicates the regular term calculation method (L1 normal form or L2 normal form), \n",
    "# C still indicates the regular term penalty coefficient.\n",
    "clf_lr_bars = GridSearchCV(lr_clf, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7294520547945206\n",
      "{'C': 10, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "clf_lr_bars.fit(X_bars, y_bars)\n",
    "print(clf_lr_bars.best_score_)\n",
    "print(clf_lr_bars.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Choose the best model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=150, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bars, X_test_bars, y_train_bars, y_test_bars = train_test_split(X_bars, y_bars, random_state=1)\n",
    "clf_lr_bars = linear_model.LogisticRegression(penalty='l2', C=150)\n",
    "clf_lr_bars.fit(X_train_bars, y_train_bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7232876712328767\n",
      "[[ 68  70]\n",
      " [ 31 196]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_bars = clf_lr_bars.predict(X_test_bars)\n",
    "accuracy = accuracy_score(y_test_bars, y_pred_bars)\n",
    "print('accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test_bars, y_pred_bars)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants\n",
    "\n",
    "***Same steps with Bars.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Too expensive for what they had... i had an eg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very rustic place. Mismatched furniture, off K...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I highly recommend Au Festin de Babette for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazing soup and dauphinoise. BUT the wait for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I went here by recommendation of a friend. Tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  Too expensive for what they had... i had an eg...      0\n",
       "1  Very rustic place. Mismatched furniture, off K...      1\n",
       "2  I highly recommend Au Festin de Babette for th...      1\n",
       "3  Amazing soup and dauphinoise. BUT the wait for...      0\n",
       "4  I went here by recommendation of a friend. Tho...      0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_restaurants = pd.read_csv(\"Restaurants.csv\")\n",
    "data_restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Pre-processing steps: Build corpus***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['too', 'expensive', 'for', 'what', 'they', 'had', '   ', 'i', 'had', 'an', 'egg', 'benedict', 'plate', 'called', 'la', 'drolet', 'but', 'it', 'had', 'a', 'weird', 'side', 'soup', 'with', 'a', 'desert', 'that', 'i', 'dont', 'personally', 'like', ' ', 'i', 'dont', 'think', 'i', 'will', 'go', 'back', 'there', 'again', '    ']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_restaurants = WordPunctTokenizer()\n",
    "corpus_res = []\n",
    "for rev in data_restaurants.review:\n",
    "    cor = []\n",
    "    for ele in tokenizer_restaurants.tokenize(rev):\n",
    "        ele = re.sub(r'[^a-zA-Z0-9]', \" \", ele)\n",
    "        cor.append(ele.lower())\n",
    "    corpus_res.append(cor)\n",
    "print(corpus_res[0: 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Numeric representation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res_path = 'review_restaurants.model'\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "model = Doc2Vec(documents, vector_size=64, window=3, min_count=1, workers=4)\n",
    "model.save(model_res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Build classification models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res = []\n",
    "for review in corpus_res:\n",
    "    X_res.append(model.infer_vector(review))\n",
    "X_res = np.array(X_res)\n",
    "y_res = data_restaurants.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1) KNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805555555555556\n",
      "{'n_neighbors': 13}\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "params = {'n_neighbors': [2 * i + 1 for i in range(8)]}\n",
    "clf_knn_res = GridSearchCV(knn, params, cv=5)\n",
    "clf_knn_res.fit(X_res, y_res)\n",
    "print(clf_knn_res.best_score_)\n",
    "print(clf_knn_res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2) Decision Tree***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6590277777777778\n",
      "{'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "dtc = tree.DecisionTreeClassifier()\n",
    "params = {'max_depth': [2 * i + 1 for i in range(8)]}\n",
    "clf_dtc_res = GridSearchCV(dtc, params, cv=5)\n",
    "clf_dtc_res.fit(X_res, y_res)\n",
    "print(clf_dtc_res.best_score_)\n",
    "print(clf_dtc_res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3) Naive Bayes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6451427821248059\n"
     ]
    }
   ],
   "source": [
    "clf_gnb = GaussianNB()\n",
    "scores = cross_val_score(clf_gnb, X_res, y_res, cv=5)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4) SVM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125\n",
      "{'C': 100, 'degree': 2, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(gamma='auto')\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly'), 'C': [1, 10, 100, 150], 'degree': [2, 3, 4]}\n",
    "clf_svm_res = GridSearchCV(svc, parameters, cv=5)\n",
    "clf_svm_res.fit(X_res, y_res)\n",
    "print(clf_svm_res.best_score_)\n",
    "print(clf_svm_res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5) Logistic Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7180555555555556\n",
      "{'C': 100, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "lr_clf = linear_model.LogisticRegression()\n",
    "parameters = {'penalty': ['l1', 'l2'], 'C': [1, 10, 100, 150]}\n",
    "clf_lr_res = GridSearchCV(lr_clf, parameters, cv=5)\n",
    "clf_lr_res.fit(X_res, y_res)\n",
    "print(clf_lr_res.best_score_)\n",
    "print(clf_lr_res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Choose the best model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=150, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res, random_state=1)\n",
    "clf_lr_res = linear_model.LogisticRegression(penalty='l2', C=150)\n",
    "clf_lr_res.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.725\n",
      "[[ 51  76]\n",
      " [ 23 210]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_res = clf_lr_res.predict(X_test_res)\n",
    "accuracy = accuracy_score(y_test_res, y_pred_res)\n",
    "print('accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test_res, y_pred_res)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "#### Evaluate the model's ability of transfering between two categories.\n",
    "\n",
    "It is shown that Logistic Regression worked best on the Bars dataset, with accuracy: 0.723, parameters {'C': 10, 'penalty': 'l1'}. And Logistic Regression with paremeters {'C': 100, 'penalty': 'l2'} worked best on the Restaurants data set with accuracy: 0.725."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Transer the classifier***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1) Train a classification model on the data from “Bars”, and evaluate its performance on the data from “Restaurants”***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.65\n",
      "[[ 99  28]\n",
      " [ 98 135]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_res = clf_lr_bars.predict(X_test_res)\n",
    "accuracy = accuracy_score(y_test_res, y_pred_res)\n",
    "print('accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test_res, y_pred_res)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***<font color=black size=2.5 bold>2) Train a classification model on the data from “Restaurants”, and evaluate its performance on the data from “Bars” </font>***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6931506849315069\n",
      "[[ 57  81]\n",
      " [ 31 196]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_bars = clf_lr_res.predict(X_test_bars)\n",
    "accuracy = accuracy_score(y_test_bars, y_pred_bars)\n",
    "print('accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test_bars, y_pred_bars)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiments, both the model works not as good as with their original training dataset. I guess the reason is that only models are transfered, they still have different word vectors, so I did another experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Transer both the doc2vec model and the classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bars = Doc2Vec.load(model_bars_path)\n",
    "model_res = Doc2Vec.load(model_res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1) Train a classification model on the data from “Bars”, and evaluate its performance on the data from “Restaurants”***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res2 = []\n",
    "for review in corpus_res:\n",
    "    X_res2.append(model_bars.infer_vector(review))\n",
    "X_res2 = np.array(X_res2)\n",
    "y_res2 = data_restaurants.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7416666666666667\n",
      "[[ 67  60]\n",
      " [ 33 200]]\n"
     ]
    }
   ],
   "source": [
    "X_train_res2, X_test_res2, y_train_res2, y_test_res2 = train_test_split(X_res2, y_res2, random_state=1)\n",
    "y_pred_res2 = clf_lr_bars.predict(X_test_res2)\n",
    "accuracy = accuracy_score(y_test_res2, y_pred_res2)\n",
    "print('accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test_res2, y_pred_res2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2) Train a classification model on the data from “Restaurants”, and evaluate its performance on the data from “Bars”***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bars2 = []\n",
    "for review in corpus_bars:\n",
    "    X_bars2.append(model_res.infer_vector(review))\n",
    "X_bars2 = np.array(X_bars2)\n",
    "y_bars2 = data_bars.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6904109589041096\n",
      "[[ 46  92]\n",
      " [ 21 206]]\n"
     ]
    }
   ],
   "source": [
    "X_train_bars2, X_test_bars2, y_train_bars2, y_test_bars2 = train_test_split(X_bars2, y_bars2, random_state=1)\n",
    "y_pred_bars2 = clf_lr_res.predict(X_test_bars2)\n",
    "accuracy = accuracy_score(y_test_bars2, y_pred_bars2)\n",
    "print('accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test_bars2, y_pred_bars2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is still not as good as I expected. Although the accuracy of the Bars data set is improved when both the model and the word vector are transferred, while the accuracy on the Restaurants data set is reduced.\n",
    "I haven't figure out the reason until I submitted the project.\n",
    "I should also try the grid search and cross validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
